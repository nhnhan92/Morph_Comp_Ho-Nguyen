{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "window_time = 700\n",
    "n_data = 20      # number of data \n",
    "n_texture = 2   # number of texture\n",
    "n_surface = 1   # number of surface for each texture\n",
    "n_mode = 3 \n",
    "n_pressure = 4\n",
    "\n",
    "header = [['Time','Strain_f'],\n",
    "          ['Time','Strain_b']]\n",
    "list_texture = ['P40_a','P40_b','P40_c','P60_a','P60_b','P60_c','P80_a','P80_b','P80_c']\n",
    "# list_texture = ['P40_a','P40_b','P40_c']\n",
    "list_pressure = [20,30,40,50]\n",
    "modes = ['forward', 'reverse', 'straight']\n",
    "folder = ['test_data_04_20']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\nhnha\\\\OneDrive\\\\Desktop\\\\Whisker paper\\\\Learning_based_research'"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "notebook_dir = os.path.dirname(os.path.realpath(\"__notebook__\"))\n",
    "parent_dir = os.path.dirname(os.path.dirname(notebook_dir))\n",
    "\n",
    "parent_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import strain data:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Original Strain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_data_forward = pd.DataFrame()\n",
    "strain_data_backward = pd.DataFrame()\n",
    "os.chdir(parent_dir + \"\\\\strain_data\\\\\"+folder[0]+\"\\\\\"+\"original\")\n",
    "\n",
    "raw_data_forward = {}\n",
    "raw_data_backward = {}\n",
    "for filename in list_texture:\n",
    "    for i in range(int(n_data/2)):    # number of data for 1 single texture in forward sweeping direction\n",
    "        raw_csv_forward = pd.read_csv(filename+'0000_I0'+f\"{i*2:03}\"+'.CSV',skiprows = 14,\n",
    "                                        header=None, names=header[0])\n",
    "                                    \n",
    "        new_header_f = \"init_\" + ''.join(filename.split(\"_\"))+\"_\"+str(2*i)\n",
    "        raw_csv_forward.rename(columns={'Strain_f': new_header_f}, inplace=True)\n",
    "        calib_forward = raw_csv_forward[new_header_f][0]\n",
    "\n",
    "        raw_csv_backward = pd.read_csv(filename+'0000_I0'+f\"{i*2+1:03}\"+'.CSV',skiprows = 14,\n",
    "                                                header=None, names=header[1])\n",
    "        new_header_b = \"init_\" + ''.join(filename.split(\"_\"))+\"_\"+str(2*i+1)                                       \n",
    "        raw_csv_backward.rename(columns={'Strain_b': new_header_b}, inplace=True)                                       \n",
    "        calib_backward = raw_csv_backward[new_header_b][0]\n",
    "        temp_forward = []\n",
    "        temp_backward = []\n",
    "        for j in range(700):\n",
    "            temp_backward.append((raw_csv_backward.iloc[250+j])[new_header_b] - calib_backward)\n",
    "            temp_forward.append((raw_csv_forward.iloc[250+j])[new_header_f] - calib_forward)\n",
    "        raw_data_forward[new_header_f] = temp_forward\n",
    "        raw_data_backward[new_header_b] = temp_backward\n",
    "            \n",
    "strain_data_forward = pd.DataFrame(raw_data_forward)\n",
    "strain_data_backward = pd.DataFrame(raw_data_backward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_data_backward\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Pressurized Strain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_forward = {}\n",
    "raw_data_backward = {}\n",
    "for mode in modes:\n",
    "    for pressure in list_pressure:\n",
    "        os.chdir(parent_dir + \"\\\\strain_data\\\\\"+folder[0]+\"\\\\\"+mode+\"\\\\P = \"+str(pressure))\n",
    "        for filename in list_texture:\n",
    "            for i in range(int(n_data/2)):    # number of data for 1 single texture in forward sweeping direction\n",
    "                raw_csv_forward = pd.read_csv(filename+'0000_I0'+f\"{i*2:03}\"+'.CSV',skiprows = 14,\n",
    "                                                header=None, names=header[0])\n",
    "                new_header_f = mode +\"_\"+str(pressure) + \"kpa_\" + ''.join(filename.split(\"_\"))+\"_\"+str(2*i)\n",
    "                raw_csv_forward.rename(columns={'Strain_f': new_header_f}, inplace=True)                                \n",
    "                calib_forward = raw_csv_forward[new_header_f][0]\n",
    "                temp_forward = []\n",
    "                for j in range(700):\n",
    "                    temp_forward.append((raw_csv_forward.iloc[250+j])[new_header_f] - calib_forward)\n",
    "                raw_data_forward[new_header_f] = temp_forward\n",
    "        \n",
    "        for filename in list_texture:\n",
    "            for i in range(int(n_data/2)):    # number of data for 1 single texture in forward sweeping direction\n",
    "                raw_csv_backward = pd.read_csv(filename+'0000_I0'+f\"{i*2+1:03}\"+'.CSV',skiprows = 14,\n",
    "                                                header=None, names=header[1])\n",
    "                new_header_b = mode +\"_\"+str(pressure) + \"kpa_\" + ''.join(filename.split(\"_\"))+\"_\"+str(2*i+1)\n",
    "                raw_csv_backward.rename(columns={'Strain_b': new_header_b}, inplace=True)                                \n",
    "                calib_backward = raw_csv_backward[new_header_b][0]\n",
    "                temp_backward = []\n",
    "                for j in range(700):\n",
    "                    temp_backward.append((raw_csv_backward.iloc[250+j])[new_header_b] - calib_backward)\n",
    "                raw_data_backward[new_header_b] = temp_backward\n",
    "                \n",
    "df_forward = pd.DataFrame(raw_data_forward)\n",
    "df_backward = pd.DataFrame(raw_data_backward)\n",
    "strain_data_forward = strain_data_forward.join(df_forward)\n",
    "strain_data_backward = strain_data_backward.join(df_backward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_data_backward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Strain DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>init_P40a_1</th>\n",
       "      <th>init_P40a_3</th>\n",
       "      <th>init_P40a_5</th>\n",
       "      <th>init_P40a_7</th>\n",
       "      <th>init_P40a_9</th>\n",
       "      <th>init_P40a_11</th>\n",
       "      <th>init_P40a_13</th>\n",
       "      <th>init_P40a_15</th>\n",
       "      <th>init_P40a_17</th>\n",
       "      <th>init_P40a_19</th>\n",
       "      <th>...</th>\n",
       "      <th>straight_50kpa_P80c_1</th>\n",
       "      <th>straight_50kpa_P80c_3</th>\n",
       "      <th>straight_50kpa_P80c_5</th>\n",
       "      <th>straight_50kpa_P80c_7</th>\n",
       "      <th>straight_50kpa_P80c_9</th>\n",
       "      <th>straight_50kpa_P80c_11</th>\n",
       "      <th>straight_50kpa_P80c_13</th>\n",
       "      <th>straight_50kpa_P80c_15</th>\n",
       "      <th>straight_50kpa_P80c_17</th>\n",
       "      <th>straight_50kpa_P80c_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115.448571</td>\n",
       "      <td>117.480000</td>\n",
       "      <td>108.080000</td>\n",
       "      <td>121.158571</td>\n",
       "      <td>105.248571</td>\n",
       "      <td>108.448571</td>\n",
       "      <td>101.974286</td>\n",
       "      <td>104.875714</td>\n",
       "      <td>104.038571</td>\n",
       "      <td>118.927143</td>\n",
       "      <td>...</td>\n",
       "      <td>180.711429</td>\n",
       "      <td>182.277143</td>\n",
       "      <td>177.420000</td>\n",
       "      <td>181.857143</td>\n",
       "      <td>185.522857</td>\n",
       "      <td>180.844286</td>\n",
       "      <td>180.258571</td>\n",
       "      <td>183.925714</td>\n",
       "      <td>181.041429</td>\n",
       "      <td>187.234286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119.442857</td>\n",
       "      <td>113.890000</td>\n",
       "      <td>109.218571</td>\n",
       "      <td>127.078571</td>\n",
       "      <td>115.088571</td>\n",
       "      <td>103.112857</td>\n",
       "      <td>89.950000</td>\n",
       "      <td>104.230000</td>\n",
       "      <td>98.987143</td>\n",
       "      <td>113.075714</td>\n",
       "      <td>...</td>\n",
       "      <td>177.630000</td>\n",
       "      <td>181.738571</td>\n",
       "      <td>177.330000</td>\n",
       "      <td>180.968571</td>\n",
       "      <td>183.490000</td>\n",
       "      <td>183.201429</td>\n",
       "      <td>183.931429</td>\n",
       "      <td>180.361429</td>\n",
       "      <td>185.711429</td>\n",
       "      <td>186.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117.781429</td>\n",
       "      <td>114.575714</td>\n",
       "      <td>99.568571</td>\n",
       "      <td>116.352857</td>\n",
       "      <td>104.204286</td>\n",
       "      <td>99.351429</td>\n",
       "      <td>104.532857</td>\n",
       "      <td>103.277143</td>\n",
       "      <td>102.367143</td>\n",
       "      <td>105.314286</td>\n",
       "      <td>...</td>\n",
       "      <td>177.252857</td>\n",
       "      <td>185.028571</td>\n",
       "      <td>181.231429</td>\n",
       "      <td>178.657143</td>\n",
       "      <td>179.045714</td>\n",
       "      <td>181.890000</td>\n",
       "      <td>186.555714</td>\n",
       "      <td>183.008571</td>\n",
       "      <td>187.981429</td>\n",
       "      <td>184.015714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>124.677143</td>\n",
       "      <td>112.487143</td>\n",
       "      <td>112.700000</td>\n",
       "      <td>121.577143</td>\n",
       "      <td>110.768571</td>\n",
       "      <td>94.698571</td>\n",
       "      <td>102.762857</td>\n",
       "      <td>113.494286</td>\n",
       "      <td>100.698571</td>\n",
       "      <td>104.671429</td>\n",
       "      <td>...</td>\n",
       "      <td>180.110000</td>\n",
       "      <td>187.017143</td>\n",
       "      <td>181.804286</td>\n",
       "      <td>181.134286</td>\n",
       "      <td>180.327143</td>\n",
       "      <td>180.785714</td>\n",
       "      <td>183.064286</td>\n",
       "      <td>185.190000</td>\n",
       "      <td>186.494286</td>\n",
       "      <td>186.061429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118.102857</td>\n",
       "      <td>100.661429</td>\n",
       "      <td>111.821429</td>\n",
       "      <td>112.421429</td>\n",
       "      <td>112.985714</td>\n",
       "      <td>108.837143</td>\n",
       "      <td>99.054286</td>\n",
       "      <td>106.522857</td>\n",
       "      <td>108.385714</td>\n",
       "      <td>96.654286</td>\n",
       "      <td>...</td>\n",
       "      <td>182.602857</td>\n",
       "      <td>184.431429</td>\n",
       "      <td>179.588571</td>\n",
       "      <td>184.935714</td>\n",
       "      <td>183.595714</td>\n",
       "      <td>184.181429</td>\n",
       "      <td>179.160000</td>\n",
       "      <td>181.232857</td>\n",
       "      <td>185.271429</td>\n",
       "      <td>188.832857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>113.544286</td>\n",
       "      <td>106.190000</td>\n",
       "      <td>113.278571</td>\n",
       "      <td>114.387143</td>\n",
       "      <td>117.018571</td>\n",
       "      <td>101.658571</td>\n",
       "      <td>101.594286</td>\n",
       "      <td>113.442857</td>\n",
       "      <td>105.895714</td>\n",
       "      <td>104.254286</td>\n",
       "      <td>...</td>\n",
       "      <td>178.242857</td>\n",
       "      <td>175.954286</td>\n",
       "      <td>177.338571</td>\n",
       "      <td>179.870000</td>\n",
       "      <td>173.947143</td>\n",
       "      <td>181.211429</td>\n",
       "      <td>176.332857</td>\n",
       "      <td>177.414286</td>\n",
       "      <td>175.135714</td>\n",
       "      <td>190.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>111.778571</td>\n",
       "      <td>101.721429</td>\n",
       "      <td>107.855714</td>\n",
       "      <td>108.552857</td>\n",
       "      <td>111.481429</td>\n",
       "      <td>108.645714</td>\n",
       "      <td>108.388571</td>\n",
       "      <td>107.358571</td>\n",
       "      <td>106.985714</td>\n",
       "      <td>105.017143</td>\n",
       "      <td>...</td>\n",
       "      <td>180.164286</td>\n",
       "      <td>177.115714</td>\n",
       "      <td>181.591429</td>\n",
       "      <td>176.820000</td>\n",
       "      <td>174.550000</td>\n",
       "      <td>179.024286</td>\n",
       "      <td>171.791429</td>\n",
       "      <td>178.660000</td>\n",
       "      <td>168.087143</td>\n",
       "      <td>187.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>107.864286</td>\n",
       "      <td>106.668571</td>\n",
       "      <td>113.842857</td>\n",
       "      <td>113.734286</td>\n",
       "      <td>110.704286</td>\n",
       "      <td>102.867143</td>\n",
       "      <td>102.650000</td>\n",
       "      <td>113.354286</td>\n",
       "      <td>101.785714</td>\n",
       "      <td>110.104286</td>\n",
       "      <td>...</td>\n",
       "      <td>179.888571</td>\n",
       "      <td>176.204286</td>\n",
       "      <td>179.268571</td>\n",
       "      <td>176.887143</td>\n",
       "      <td>181.358571</td>\n",
       "      <td>177.897143</td>\n",
       "      <td>174.072857</td>\n",
       "      <td>177.878571</td>\n",
       "      <td>174.350000</td>\n",
       "      <td>186.581429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>114.084286</td>\n",
       "      <td>100.177143</td>\n",
       "      <td>114.068571</td>\n",
       "      <td>107.244286</td>\n",
       "      <td>116.227143</td>\n",
       "      <td>110.360000</td>\n",
       "      <td>107.611429</td>\n",
       "      <td>112.605714</td>\n",
       "      <td>110.645714</td>\n",
       "      <td>105.105714</td>\n",
       "      <td>...</td>\n",
       "      <td>178.887143</td>\n",
       "      <td>174.307143</td>\n",
       "      <td>177.545714</td>\n",
       "      <td>177.802857</td>\n",
       "      <td>181.585714</td>\n",
       "      <td>177.294286</td>\n",
       "      <td>179.631429</td>\n",
       "      <td>173.934286</td>\n",
       "      <td>182.097143</td>\n",
       "      <td>187.745714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>105.165714</td>\n",
       "      <td>104.027143</td>\n",
       "      <td>108.475714</td>\n",
       "      <td>110.630000</td>\n",
       "      <td>112.301429</td>\n",
       "      <td>107.510000</td>\n",
       "      <td>106.728571</td>\n",
       "      <td>108.967143</td>\n",
       "      <td>106.345714</td>\n",
       "      <td>112.584286</td>\n",
       "      <td>...</td>\n",
       "      <td>177.855714</td>\n",
       "      <td>177.724286</td>\n",
       "      <td>177.665714</td>\n",
       "      <td>173.531429</td>\n",
       "      <td>177.432857</td>\n",
       "      <td>176.784286</td>\n",
       "      <td>180.317143</td>\n",
       "      <td>177.991429</td>\n",
       "      <td>179.972857</td>\n",
       "      <td>184.532857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 1170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     init_P40a_1  init_P40a_3  init_P40a_5  init_P40a_7  init_P40a_9  \\\n",
       "0     115.448571   117.480000   108.080000   121.158571   105.248571   \n",
       "1     119.442857   113.890000   109.218571   127.078571   115.088571   \n",
       "2     117.781429   114.575714    99.568571   116.352857   104.204286   \n",
       "3     124.677143   112.487143   112.700000   121.577143   110.768571   \n",
       "4     118.102857   100.661429   111.821429   112.421429   112.985714   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "695   113.544286   106.190000   113.278571   114.387143   117.018571   \n",
       "696   111.778571   101.721429   107.855714   108.552857   111.481429   \n",
       "697   107.864286   106.668571   113.842857   113.734286   110.704286   \n",
       "698   114.084286   100.177143   114.068571   107.244286   116.227143   \n",
       "699   105.165714   104.027143   108.475714   110.630000   112.301429   \n",
       "\n",
       "     init_P40a_11  init_P40a_13  init_P40a_15  init_P40a_17  init_P40a_19  \\\n",
       "0      108.448571    101.974286    104.875714    104.038571    118.927143   \n",
       "1      103.112857     89.950000    104.230000     98.987143    113.075714   \n",
       "2       99.351429    104.532857    103.277143    102.367143    105.314286   \n",
       "3       94.698571    102.762857    113.494286    100.698571    104.671429   \n",
       "4      108.837143     99.054286    106.522857    108.385714     96.654286   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "695    101.658571    101.594286    113.442857    105.895714    104.254286   \n",
       "696    108.645714    108.388571    107.358571    106.985714    105.017143   \n",
       "697    102.867143    102.650000    113.354286    101.785714    110.104286   \n",
       "698    110.360000    107.611429    112.605714    110.645714    105.105714   \n",
       "699    107.510000    106.728571    108.967143    106.345714    112.584286   \n",
       "\n",
       "     ...  straight_50kpa_P80c_1  straight_50kpa_P80c_3  straight_50kpa_P80c_5  \\\n",
       "0    ...             180.711429             182.277143             177.420000   \n",
       "1    ...             177.630000             181.738571             177.330000   \n",
       "2    ...             177.252857             185.028571             181.231429   \n",
       "3    ...             180.110000             187.017143             181.804286   \n",
       "4    ...             182.602857             184.431429             179.588571   \n",
       "..   ...                    ...                    ...                    ...   \n",
       "695  ...             178.242857             175.954286             177.338571   \n",
       "696  ...             180.164286             177.115714             181.591429   \n",
       "697  ...             179.888571             176.204286             179.268571   \n",
       "698  ...             178.887143             174.307143             177.545714   \n",
       "699  ...             177.855714             177.724286             177.665714   \n",
       "\n",
       "     straight_50kpa_P80c_7  straight_50kpa_P80c_9  straight_50kpa_P80c_11  \\\n",
       "0               181.857143             185.522857              180.844286   \n",
       "1               180.968571             183.490000              183.201429   \n",
       "2               178.657143             179.045714              181.890000   \n",
       "3               181.134286             180.327143              180.785714   \n",
       "4               184.935714             183.595714              184.181429   \n",
       "..                     ...                    ...                     ...   \n",
       "695             179.870000             173.947143              181.211429   \n",
       "696             176.820000             174.550000              179.024286   \n",
       "697             176.887143             181.358571              177.897143   \n",
       "698             177.802857             181.585714              177.294286   \n",
       "699             173.531429             177.432857              176.784286   \n",
       "\n",
       "     straight_50kpa_P80c_13  straight_50kpa_P80c_15  straight_50kpa_P80c_17  \\\n",
       "0                180.258571              183.925714              181.041429   \n",
       "1                183.931429              180.361429              185.711429   \n",
       "2                186.555714              183.008571              187.981429   \n",
       "3                183.064286              185.190000              186.494286   \n",
       "4                179.160000              181.232857              185.271429   \n",
       "..                      ...                     ...                     ...   \n",
       "695              176.332857              177.414286              175.135714   \n",
       "696              171.791429              178.660000              168.087143   \n",
       "697              174.072857              177.878571              174.350000   \n",
       "698              179.631429              173.934286              182.097143   \n",
       "699              180.317143              177.991429              179.972857   \n",
       "\n",
       "     straight_50kpa_P80c_19  \n",
       "0                187.234286  \n",
       "1                186.430000  \n",
       "2                184.015714  \n",
       "3                186.061429  \n",
       "4                188.832857  \n",
       "..                      ...  \n",
       "695              190.890000  \n",
       "696              187.640000  \n",
       "697              186.581429  \n",
       "698              187.745714  \n",
       "699              184.532857  \n",
       "\n",
       "[700 rows x 1170 columns]"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "strain_data = {}\n",
    "strain_data['odd'] = strain_data_backward\n",
    "strain_data['even'] = strain_data_forward\n",
    "strain_data['odd']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "lower = 20\n",
    "upper = 50\n",
    "FEATURES_RANGE = range(lower,upper)\n",
    "state = 'init'\n",
    "os.chdir(parent_dir + \"\\\\data_processing\\\\pre-processing\\\\\")\n",
    "X = []\n",
    "labels = []\n",
    "label_map = {}\n",
    "for type_file in ['odd','even']:\n",
    "    label_map = dict([\n",
    "        (v, i) for i, v in enumerate(\n",
    "            list(set(\"_\".join(e.split(\"_\")[:-1]) for e in strain_data[type_file])) # remove the last component in file name (*_00, *_01, ...)\n",
    "        )])\n",
    "    \n",
    "    X = [strain_data[type_file][header][FEATURES_RANGE] for header in strain_data[type_file] if state in header]\n",
    "    # labels = [header.join(e.split(\"_\")[:-1]) for header in strain_data[type_file] if state in header]\n",
    "    labels = [\"_\".join(header.split(\"_\")[:-1]) for header in strain_data[type_file] if state in header]\n",
    "\n",
    "    \n",
    "    reduce_processor = PCA(n_components=2)\n",
    "    pipe = Pipeline([('pca', reduce_processor)])\n",
    "    X_reduced = pipe.fit_transform(X)\n",
    "\n",
    "    plot, ax = plt.subplots()\n",
    "    figure = ax.scatter(X_reduced[:,0], X_reduced[:,1], \n",
    "            # alpha=hue,\n",
    "            c=[label_map[\"_\".join(e.split(\"_\")[:])] for e in labels]\n",
    "            )\n",
    "\n",
    "    for i, txt in enumerate(labels):\n",
    "        ax.text(X_reduced[i,0]+.04, X_reduced[i,1]+.04, txt, fontsize=6)\n",
    "    plot.show()\n",
    "\n",
    "    plot.savefig(f'{str(reduce_processor)[:3]}_{state}{lower}-{upper}_{type_file}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"_\".join(header.split(\"_\")[:-1]) for header in strain_data[type_file] if state in header]\n",
    "# labels = strain_data[type_file].split(\"_\")[:-1]\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Look up range\n",
    "lower = 20\n",
    "upper = 50\n",
    "FEATURES_RANGE = range(lower,upper)\n",
    "pressure = '30kpa'\n",
    "\n",
    "state = 'straight'\n",
    "os.chdir(parent_dir + \"\\\\data_processing\\\\pre-processing\\\\\")\n",
    "X = []\n",
    "labels = []\n",
    "label_map = {}\n",
    "for type_file in ['even','odd']:\n",
    "    \n",
    "    X = [strain_data[type_file][header][FEATURES_RANGE] for header in strain_data[type_file] if state in header and pressure in header ]\n",
    "    # labels += [header for header in strain_data[type_file] if state in header and pressure in header]\n",
    "    labels = [\"_\".join(header.split(\"_\")[1:-1]) for header in strain_data[type_file] if state in header and pressure in header]\n",
    "    label_map.update(dict([(v, i) for i, v in enumerate(list(set(\"_\".join(e.split(\"_\")[1:-1]) for e in strain_data[type_file])))]))\n",
    "\n",
    "    reduce_processor = PCA(n_components=2)\n",
    "    pipe = Pipeline([('pca', reduce_processor)])\n",
    "    X_reduced = pipe.fit_transform(X)\n",
    "\n",
    "    plot, ax = plt.subplots()\n",
    "    figure = ax.scatter(X_reduced[:,0], X_reduced[:,1], \n",
    "            # alpha=hue,\n",
    "            c=[label_map[\"_\".join(e.split(\"_\")[1:])] for e in labels]\n",
    "            )\n",
    "\n",
    "    for i, txt in enumerate(labels):\n",
    "        ax.text(X_reduced[i,0]+.04, X_reduced[i,1]+.04, txt, fontsize=5)\n",
    "\n",
    "    plot.show()\n",
    "\n",
    "    plot.savefig(f'{str(reduce_processor)[:3]}_{state}{lower}-{upper}_{pressure}_{type_file}.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)\n",
    "# label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Fourier Transform (FFT)\n",
    "def FFT(dataset, plot):\n",
    "    global dominate_freq\n",
    "    dt = 1/100     # tần số lấy mẫu = 100Hz\n",
    "    t = np.arange(0,7,dt)\n",
    "    n = len(t)\n",
    "    lowpass_order_filltered = 1\n",
    "    highpass_order_filltered = 0\n",
    "    fhat = np.fft.fft(dataset,n)\n",
    "    mag = np.abs(fhat)/n  # chia cho n =  normalization in the spectral domain \n",
    "    PSD = fhat * np.conj(fhat)/n\n",
    "    L = np.arange(1,np.floor(n/2),dtype='int')\n",
    "    freq = (1/(dt*n))*np.arange(n) \n",
    "    centroid = np.sum(freq[lowpass_order_filltered:(350-highpass_order_filltered)]\n",
    "                      *mag[lowpass_order_filltered:(350-highpass_order_filltered)])/np.sum(mag[lowpass_order_filltered:(350-highpass_order_filltered)])\n",
    "    sum_energy = np.sum(mag[lowpass_order_filltered:(len(L)-highpass_order_filltered)])\n",
    "    weight_mean_amplitude = np.sum(freq[lowpass_order_filltered:(350-highpass_order_filltered)]\n",
    "                      *mag[lowpass_order_filltered:(350-highpass_order_filltered)])/np.sum(freq[lowpass_order_filltered:(350-highpass_order_filltered)])\n",
    "    \n",
    "    max_mag = max(mag[lowpass_order_filltered:-1])   \n",
    "    for i in range(lowpass_order_filltered, len(freq)): \n",
    "        if abs(mag[i] - max_mag) < 0.00000001:\n",
    "            dominate_freq = freq[i]\n",
    "            break\n",
    "    if plot == True:\n",
    "        fig, axs = plt.subplots(2,1,sharex=False, sharey=False)\n",
    "        plt.close() \n",
    "        plt.rcParams['figure.figsize'] = [7,7]\n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "        plt.sca(axs[0])\n",
    "        plt.plot(t,dataset,linewidth=2, color='c', label='Time_Domain')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Magnitude')\n",
    "\n",
    "        plt.sca(axs[1])\n",
    "        plt.plot(freq[L],mag[L],color='c', linewidth=2, label='Frequency_Domain')\n",
    "        # plt.plot([centroid]*len(freq[lowpass_order_filltered:len(L)]),mag[lowpass_order_filltered:len(L)],color='r', linewidth=2)\n",
    "#         plt.plot(freq[L],[energy]*len(L))\n",
    "        # plt.text(centroid,0,'Spectral centroid = '+ str(centroid), va = 'top')\n",
    "        # plt.text(25,max_mag/2,'Sum energy = '+ str(sum_energy), va = 'top')\n",
    "        plt.text(dominate_freq,max_mag,'dominate frequency = '+ str(dominate_freq), va = 'top')\n",
    "        plt.xlim(freq[L[0]],freq[L[-1]])\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        plt.ylabel('Magnitude')\n",
    "        plt.show()\n",
    "    \n",
    "    return dominate_freq, freq, mag, max_mag, centroid, sum_energy, fhat\n",
    "\n",
    "def freq_filter(dataset, low_cut_freq, high_cut_freq,plot):\n",
    "    dt = 1/100     # tần số lấy mẫu = 100Hz\n",
    "    t = np.arange(0,7,dt)\n",
    "    n = len(t)\n",
    "    dominate_freq, freq, mag, max_mag, centroid, energy, fhat = FFT(dataset,plot = False)\n",
    "    indices = np.logical_or(np.logical_and(freq>low_cut_freq, freq<high_cut_freq), \n",
    "                            np.logical_and(freq>100-high_cut_freq, freq<100-low_cut_freq))\n",
    "    indices[0] = True\n",
    "    L = np.arange(1,np.floor(n/2),dtype='int')\n",
    "    clean_mag = mag*indices\n",
    "    fhat = indices * fhat\n",
    "\n",
    "    ffilt = (np.fft.ifft(fhat)).real\n",
    "    if plot == True:\n",
    "        fig, axs = plt.subplots(2,1,sharex=False, sharey=False)\n",
    "        plt.sca(axs[0])\n",
    "        plt.plot(t,ffilt,linewidth=2, color='c', label='Filtered')\n",
    "        plt.plot(np.arange(0,7,dt),dataset, color='r', linewidth=2,label='Raw')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Magnitude')\n",
    "        \n",
    "        plt.sca(axs[1])\n",
    "        plt.plot(freq[L],clean_mag[L],color='c', linewidth=2, label = 'Filtered')\n",
    "        plt.plot(freq[L],mag[L],color='r', linewidth=2, label = 'Raw')\n",
    "        plt.xlim(freq[L[0]],freq[L[-1]])\n",
    "        plt.legend()\n",
    "        plt.xlabel('Frequency')\n",
    "        plt.ylabel('Magnitude')\n",
    "        \n",
    "        fig.add_subplot(111, frameon = False)\n",
    "        plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "        plt.title('Comparison data in time domain after filtering' )\n",
    "    return ffilt, clean_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "sc_maxabs = preprocessing.MaxAbsScaler()\n",
    "tempo_data =[]\n",
    "header = ['Strain_f','Strain_b']\n",
    "sc_maxabs.fit(strain_data[['Strain_f','Strain_b']])\n",
    "strain_data_scaled = sc_maxabs.transform(strain_data[['Strain_f','Strain_b']])\n",
    "strain_data_scaled = (pd.DataFrame(strain_data_scaled, columns=header)).assign(Time = total_time)\n",
    "\n",
    "strain_data_scaled\n",
    "\n",
    "### No normalization\n",
    "k = 0\n",
    "for filename in list_texture:\n",
    "    for i in range(int(n_data/2)):\n",
    "        strain_data[700*(10*k+i):700*(10*k+i+1)].to_csv(filename+'_'+f\"{i * 2:02}\"+'.csv',columns=['Strain_f'], index=0)\n",
    "        strain_data[700*(10*k+i):700*(10*k+i+1)].to_csv(filename+'_'+f\"{i * 2+1:02}\"+'.csv',columns=['Strain_b'], index=0)\n",
    "    k += 1\n",
    "\n",
    "### With normalization\n",
    "k = 0\n",
    "for filename in list_texture:\n",
    "    for i in range(int(n_data/2)):\n",
    "        strain_data_scaled[700*(10*k+i):700*(10*k+i+1)].to_csv(filename+'_'+f\"{i * 2:02}\"+'_scaled.csv',columns=['Strain_f'], index=0)\n",
    "        strain_data_scaled[700*(10*k+i):700*(10*k+i+1)].to_csv(filename+'_'+f\"{i * 2+1:02}\"+'_scaled.csv',columns=['Strain_b'], index=0)\n",
    "    k += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominate_freq, freq, mag, max_mag, centroid, energy, fhat = FFT(strain_data_scaled[header[0]][0:700],plot = 1)\n",
    "freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for m in header:\n",
    "    tempo_data =[]\n",
    "    for i in range(n_texture):\n",
    "        for j in range(n_surface):\n",
    "            for k in range(int(n_data/2)): \n",
    "                index = range(window_time*(int(n_data*n_surface*i/2)+int(n_data*j/2)+k),window_time*(int(n_data*n_surface*i/2)+int(n_data*j/2)+k+1))\n",
    "                tempo_data.extend((freq_filter(strain_data_scaled[m][index],0.66666667,30, plot=0))[0])\n",
    "    data  = data + [tempo_data]\n",
    "\n",
    "processed_data = []\n",
    "for i in range(len(data[0])):\n",
    "    post_strain_data = []\n",
    "    for j in range(len(header)):\n",
    "        post_strain_data.append(data[j][i]) \n",
    "    processed_data.extend([post_strain_data])\n",
    "processed_strain_data = (pd.DataFrame(processed_data, columns=header)).assign(Time = total_time)\n",
    "processed_strain_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_filter(strain_data_scaled['Strain_f'][0:700],0.2,40, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(0,7,dt)\n",
    "dataset = strain_data['Strain_f'][0:700]\n",
    "dataset1 = strain_data['Strain_f'][2100:2800]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2,1,sharex=False, sharey=False)\n",
    "plt.close() \n",
    "plt.rcParams['figure.figsize'] = [7,7]\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "plt.sca(axs[0])\n",
    "plt.plot(t,dataset,linewidth=2, color='c', label='P40_origin')\n",
    "plt.plot(t,dataset1,linewidth=2, color='r', label='P60_origin')\n",
    "plt.legend()\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Magnitude')\n",
    "\n",
    "dominate_freq, freq, mag, max_mag, centroid, energy, fhat = FFT(dataset,plot = False)\n",
    "dominate_freq1, freq1, mag1, max_mag1, centroid1, energy1, fhat1 = FFT(dataset1,plot = False)\n",
    "plt.sca(axs[1])\n",
    "plt.plot(freq[L],mag[L],color='c', linewidth=2, label='Frequency_Domain')\n",
    "# plt.text(dominate_freq,max_mag,'dominate frequency = '+ str(dominate_freq1), va = 'top')\n",
    "plt.plot(freq1[L],mag1[L],color='r', linewidth=2, label='Frequency_Domain')\n",
    "# plt.text(dominate_freq1,max_mag1,'dominate frequency = '+ str(dominate_freq1), va = 'top')\n",
    "plt.xlim(freq[L[0]],freq[L[-1]])\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('C:/Users/nhnha/OneDrive/Desktop/Whisker paper/Living-machine2023/data_processing/pre-processing/ss_respone_origin.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(0,7,dt)\n",
    "dataset = strain_data['Strain_f'][0:700]\n",
    "dataset1 = strain_data['Strain_f'][14000:14700]\n",
    "dataset2 = strain_data['Strain_f'][21000:21700]\n",
    "dataset3 = strain_data['Strain_f'][28000:28700]\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2,1,sharex=False, sharey=False)\n",
    "plt.close() \n",
    "plt.rcParams['figure.figsize'] = [7,7]\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "plt.sca(axs[0])\n",
    "plt.plot(t,dataset,linewidth=2, color='c', label='P40_forward_20')\n",
    "plt.plot(t,dataset1,linewidth=2, color='r', label='P40_forward_20')\n",
    "plt.plot(t,dataset2,linewidth=2, color='b', label='P40_forward_30')\n",
    "plt.plot(t,dataset3,linewidth=2, color='g', label='P40_forward_40')\n",
    "plt.legend()\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Magnitude')\n",
    "\n",
    "dominate_freq, freq, mag, max_mag, centroid, energy, fhat = FFT(dataset,plot = False)\n",
    "dominate_freq1, freq1, mag1, max_mag1, centroid1, energy1, fhat1 = FFT(dataset1,plot = False)\n",
    "dominate_freq2, freq2, mag2, max_mag2, centroid2, energy2, fhat2 = FFT(dataset1,plot = False)\n",
    "dominate_freq3, freq3, mag3, max_mag3, centroid3, energy3, fhat3 = FFT(dataset1,plot = False)\n",
    "plt.sca(axs[1])\n",
    "plt.plot(freq[L],mag[L],color='c', linewidth=2, label='Frequency_Domain')\n",
    "plt.plot(freq1[L],mag1[L],color='r', linewidth=2, label='Frequency_Domain')\n",
    "plt.plot(freq2[L],mag2[L],color='b', linewidth=2, label='Frequency_Domain')\n",
    "plt.plot(freq2[L],mag2[L],color='g', linewidth=2, label='Frequency_Domain')\n",
    "plt.xlim(freq[L[0]],freq[L[-1]])\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('C:/Users/nhnha/OneDrive/Desktop/Whisker paper/Living-machine2023/data_processing/pre-processing/ss_respone_forward_20.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(0,7,dt)\n",
    "dataset = strain_data['Strain_b'][0:700]\n",
    "dataset1 = strain_data['Strain_b'][14000:14700]\n",
    "dataset2 = strain_data['Strain_b'][21000:21700]\n",
    "dataset3 = strain_data['Strain_b'][28000:28700]\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2,1,sharex=False, sharey=False)\n",
    "plt.close() \n",
    "plt.rcParams['figure.figsize'] = [7,7]\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "plt.sca(axs[0])\n",
    "plt.plot(t,dataset,linewidth=2, color='c', label='P40_backward_20')\n",
    "plt.plot(t,dataset1,linewidth=2, color='r', label='P40_backward_20')\n",
    "plt.plot(t,dataset2,linewidth=2, color='b', label='P40_backward_30')\n",
    "plt.plot(t,dataset3,linewidth=2, color='g', label='P40_backward_40')\n",
    "plt.legend()\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Magnitude')\n",
    "\n",
    "dominate_freq, freq, mag, max_mag, centroid, energy, fhat = FFT(dataset,plot = False)\n",
    "dominate_freq1, freq1, mag1, max_mag1, centroid1, energy1, fhat1 = FFT(dataset1,plot = False)\n",
    "dominate_freq2, freq2, mag2, max_mag2, centroid2, energy2, fhat2 = FFT(dataset1,plot = False)\n",
    "dominate_freq3, freq3, mag3, max_mag3, centroid3, energy3, fhat3 = FFT(dataset1,plot = False)\n",
    "plt.sca(axs[1])\n",
    "plt.plot(freq[L],mag[L],color='c', linewidth=2, label='Frequency_Domain')\n",
    "plt.plot(freq1[L],mag1[L],color='r', linewidth=2, label='Frequency_Domain')\n",
    "plt.plot(freq2[L],mag2[L],color='b', linewidth=2, label='Frequency_Domain')\n",
    "plt.plot(freq2[L],mag2[L],color='g', linewidth=2, label='Frequency_Domain')\n",
    "plt.xlim(freq[L[0]],freq[L[-1]])\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('C:/Users/nhnha/OneDrive/Desktop/Whisker paper/Living-machine2023/data_processing/pre-processing/ss_respone_backward_20.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sofapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f83dddd15ea16496faf7a5229a89961a784392df9f18356d4c62254781a340f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
